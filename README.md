# fashtion_mnist
# X：[256,784],  W:[784,10]  ,b：[10] ，这样得出的y_hat就是[256,10]  # 即256个样本，每一行10个类别对应着10个结果的概率（其中每一行加起来等于1）  # 在动手学深度学习中，有句话，交叉嫡只关心对正确类别的预测概率，因为只要其值足够大，就能保证分类是正确的  # y_hat.gather(1, y.view(-1, 1)) 是一个[256,1] 即存着正确分类的概率，那为什么要加上负号呢  # torch.log(torch.Tensor([0.0829]))=-2.4901 # torch.log(torch.Tensor([0.9]))=-0.1054  我们的目的是出现更多的0.9以上的概率（小于1）这样得出值更大 # 加上负号之后就是能让值更小了，还理解不清楚的话就是他们的作用都是离0更近了
